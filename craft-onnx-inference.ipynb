{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f5946c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Copyright (c) 2019-present NAVER Corp.\n",
    "MIT License\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import craft_utils\n",
    "import imgproc\n",
    "import file_utils\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from craft import CRAFT\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3de310",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"craft_mlt_25k.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e79af00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.1187,  1.1015,  1.0844,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 1.1015,  1.1015,  1.1015,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [ 1.1015,  1.1015,  1.1015,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "         [[ 1.1331,  1.1155,  1.0980,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 1.1155,  1.1155,  1.0980,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [ 1.1155,  1.1155,  1.1155,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "         [[ 1.9951,  1.9777,  1.9603,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 1.9777,  1.9777,  1.9603,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [ 1.9777,  1.9777,  1.9777,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy\n",
    "import cv2\n",
    "\n",
    "# Load the ONNX model into memory\n",
    "sess = ort.InferenceSession(model_path)\n",
    "\n",
    "image = cv2.imread(\"E:/Projects/DATA/license_plate/plates/18.jpg\")\n",
    "img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)\n",
    "ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "# preprocessing\n",
    "x = imgproc.normalizeMeanVariance(img_resized)\n",
    "x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "\n",
    "# Prepare the input data\n",
    "input_data = {\n",
    "    \"input\": x\n",
    "}\n",
    "\n",
    "# Run the model\n",
    "output_data = sess.run(None, input_data)\n",
    "\n",
    "# Postprocess the output data\n",
    "print(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c50c5689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [1, 1, 32, 100]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model into memory\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Print the shape of the input tensor\n",
    "input_shape = model.graph.input[0].type.tensor_type.shape.dim\n",
    "print(\"Input shape:\", [dim.dim_value for dim in input_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd8569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e6020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3307a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e847689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "craft-pytorch",
   "language": "python",
   "name": "craft-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
